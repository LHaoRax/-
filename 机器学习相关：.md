## 机器学习相关：

### 特征工程方面：

1. 常见的机器学习指标，解释准确率、recall、precision、ROC曲线和AUC、KS曲线及KS值
    1. 准确率:对于分类问题,准确率accuracy为所有分类正确的样本数量除以总样本数量,也就是混淆矩阵的对角线和除以总数.这个指标的问题在于对于不均衡样本无法很好的反映模型精度.
    2. recall:召回率.计算公式为$\frac{TP}{TP + FN}$.分子是指分类正确的正样本,分母是指所有真实的正样本.因此召回率就是计算所有真实正样本中有多少被正确识别出来的.
    3. precision:精确度.计算公式为$\frac{TP}{TP+FP}$.分子是指分类正确的正样本,分母是所有被分为正样本的样本,因此精确度计算的是在所有分类为正样本的样本中,有多少是真正的正样本.
    4. F1Score:recall和precision的调和平均.$\frac{1}{F1} = \frac{1}{\frac{1}{recall} + \frac{1}{precision}}$即$F1 = \frac{2recall * precision}{recall + precision}$
    5. ROC曲线:ROC曲线的横轴是FPR,公式为$\frac{FP}{FP+TN}$,指的是所有真实的负样本中有多少被错误分类到正样本中.纵轴是TPR也就是recall.对于一个模型,会通过修改模型分类阈值形成一条ROC曲线,例如LR,可以通过修改正负分类阈值0.5这个值,对于不同的值会获得一个对应的(FPR, TPR)pairs,将这些pairs连成线则形成了一条ROC.因此不同的模型可以形成不同的ROC曲线.而AUC是指ROC曲线下的面积.相当于ROC曲线的量化指标,AUC越大,模型越好.
    6. KS曲线:KS曲线的横坐标为阈值,纵轴为TPR和FPR,也就是说一个KS图上是有两条曲线的,KS值即为TPR曲线和FPR曲线之间的最大差值
2. 样本数量不平衡时的处理方式(正样本少,负样本多)
    1. 采用上采样的方法.对于样本少的数据可以采用上采样,即让样本进行复制,但是问题是这样的复制方式会引入大量的重复数据,因此需要在复制的时候给样本增加随机扰动;
    2. 采用下采样的方法.对样本多的数据可以采用下采样,即减少负样本的数量使其和正样本数量均衡.但这样的问题是模型能够学习到的数据空间减小,模型泛化能力变差,具体的改进方法可以在模型阶段进行改善,例如用randomforest的思想或者,增量训练的思想.
    3. 采用数据生成的方式SMOTE.在邻域内随机选择一个点来生成新的点.存在两个问题:1.生成的点没有足够多的有益信息;2.会生成一些重合的样本点
    4. 通过模型来改善,例如树模型或者增量模型等.
3. 为什么特征解可以描述主成分
4. 过拟合以及过拟合产生的原因

### 模型方面

1. 最小二乘法公式推导

2. Logistics原理及推导

    y = wx+b加一个sigmoid将输出的范围映射到0-1之间.

3. 逻辑回归为什么要特征离散化
    1. 稀疏向量内积乘法运算速度快,计算结果方便存储,容易扩展
    2. 离散化后的特征对异常数据有很强的鲁棒性
    3. 逻辑回归本身的表达能力有限,将单变量离散化为N个后,每个变量有单独的权重,引入了非线性,提升模型的表达能力
    4. 离散化后可以进行特征交叉,进一步引入非线性

4. 梯度下降法找到的一定是下降最快的方向么
    不一定,只是当前切平面的切线方向.

5. 梯度下降法,牛顿法,拟牛顿法的区别
    1. 梯度下降法是一阶优化方法,牛顿法是二阶优化的方法,需要计算Hesson阵.具体来说梯度下降法是一种迭代算法,选取适当的初值,不断根据负梯度的值进行迭代,直到收敛.
    2. 牛顿法每一步需要计算Hesse阵的逆矩阵,因为极值发生在一阶导数为0的点.牛顿法只适用于Hesse阵正定的情况,在深度学习中,如果Hesse阵的特征值不都是正的,会导致更新向错误方向移动,可以通过正则化避免,常用的正则化方式是在对角线上增加单位量.
    3. 拟牛顿法的出现是为了改进Hesse阵计算过程中的复杂度过高的问题.用一个n阶矩阵来近似Hesse阵的拟矩阵,其需要满足拟牛顿条件.

6. SVM的主要思想

    建立一个最优决策超平面,使得分类正负样本距离这个超平面距离最大化

7. 手推SVM损失函数

    损失函数来自于满足分类正确性的同时最大化几何间隔

8. 如何理解SVM中的函数间隔和几何间隔
    1. 函数间隔:通过观察wx+b与label y的符号是否一致可判断分类是否正确,y(wx+b).但是函数间隔存在一定的问题,如果成比例改变w和b,函数间隔的值将随之改变.
    2. 几何间隔:函数间隔除以||w||,直观上的点到平面的距离

9. 什么是支持向量

    间隔边界上的点是支持向量,满足$y_i(w^Tx_i+b)=1$

10. SVM中的核函数种类

    1. 多项式核
    2. 高斯核,由$\sigma$参数控制,如果其过小,那么可以讲任意的数据映射为线性可分,但容易过拟合
    3. 线性核
    4. 核函数的价值在于虽然是将特征进行从低维到高维的转换,但它避免了在高维空间上的复杂计算,而是先在低维上进行计算,再将分类效果表现在高维上

11. SVM为什么要引入对偶

    1. 对偶问题将原始问题的不等式约束转化为对偶问题中的等式约束,更容易求解
    2. 方便核函数的引入

12. SVM中kernel的作用

    把低维数据映射到高维,使得某些低维不可分的问题可以在高维解决.

13. LR和SVM损失函数的区别

    LR采用的是最大似然估计,SVM采用的是合页损失函数

14. L1正则和L2正则什么分布，数学上如何解释，如何选择

    1. L1是拉普拉斯分布, 是不完全可微的,在参数W=0点的概率最高;L2是高斯分布,是完全可微的,在9附近的概率较大
    2. L1使特征稀疏,在一定程度上实现了对特征的选择;
    3. L2只会将对模型贡献不大的特征所对应的参数置于无限小的值,因为参数值很小概率会出现在坐标轴上

15. VC维的含义

16. 防止过拟合的方式
    1. 正则化
    2. 增加数据量,减少特征维度
    3. Early Stop
    4. Dropout
    5. BN

17. 为什么要做特征归一化,作用是什么
    1. 归一化可以加快梯度下降法求解最优解的速度,当数值范围相差太大时,收敛路径呈Z字型,导致收敛太慢.
    2. 归一化可以提高计算精度.对于KNN来说,如果某个特征范围非常大,那么距离计算就主要取决于这个特征,从而与实际情况互相违背.

18. 哪些机器学习模型不需要归一化

    通常树模型不需要归一化,因为归一化并不会对树模型的特征划分产生印象

19. KNN中的K如何选取

    1. K值较小,模型复杂度较高,容易发生过拟合
    2. K值较大,模型复杂度下降
    3. 通常用交叉验证法来选取最优K值

20. 参数模型及非参数模型

21. 参数模型

22. 朴素贝叶斯相关公式

23. 超参优化的方式

24. 分类树模型的评价指标

     1. 熵
     2. 信息增益
     3. 信息增益率
     4. 基尼指数

25. 回归树的评价指标

    1. 均方误差
    2. 对数误差

26. 回归树的分裂方式

    遍历所有特征的所有切分点,找到最优切分点

27. Adaboost权值更新公式

28. 决策树的剪枝方式

29. boosting和bagging在不同情况下的选用

30. GBDT推导

31. GBDT和randomforeast的区别

     1. GBDT是boosting方法,每一次训练都是为了减少上一次的残差,在GBDT中,关键就是利用损失函数的负梯度在当前模型的值作为残差的近似值,进而拟合一颗CART回归树,因为GBDT会累加所有树的结果,因此只能是回归树
     2. RF是bagging的一种,各个基分类器之间并列生成.RF是以决策树为基分类器构建,在训练过程中引入随机特征选择.
     3. RF主要包含四个部分:随机选择样本(有放回);随机选择特征;构建决策树;平均投票
     4. RF的重要特征就是不需要进行交叉验证,因为只是用了一部分样本
     5. 相比于普通的bagging,RF的训练效率较高,但是起始性能交叉.

32. 介绍GBDT和xgboost的区别及联系

     1. 传统的GBDT以CART树作为基分类器,XGBoost支持线性分类器,此时XGBoost相当于正则化的LR和线性回归
     2. GBDT在优化的时候只用到了一阶导数信息,XGBoost则对代价函数进行了二阶泰勒展开,得到了一阶及二阶导数
     3. XGBoost在代价函数中加入了正则项,用于控制模型的复杂度,防止过拟合
     4. shrinkage(缩减),相当于学习速率,在进行完一次迭代时候,会将叶子节点的权值上乘上该系数,为了削弱每棵树的影响
     5. 列抽样.借鉴随机森林的做法,支持列抽样
     6. 对缺失值的处理
     7. 并行化.是在特征粒度上的并行,使用了block结构,预先对数据进行排序.

33. GBDT和xgboost在划分时有什么区别

    1. xgboost划分是通过对每一个节点都遍历所有的特征,先按照该特征里面的值进行排序,然后线性扫描进而确定最好的分割点
    2. 对于所有的特征x，只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和GL和GR。然后用计算Gain的公式计算每个分割方案的分数就可以了。
    3. 引入分割不一定会使得情况变好，所以我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值γ 的时候，则忽略这个分割

34. xgboost的原理

35. xgboost求节点值的方式

     二次规划求最优

36. 为什么xgboost要用泰勒展开

     1. 二阶导数为了梯度下降的更快
     2. 自定义loss function

37. xgboost如何处理缺省值

     1. 把缺失值当做稀疏矩阵来对待,本身在节点分裂时不考虑缺失值的数值,会分到左子树和右子树分别计算层损失,选择较优的一个.
     2. 如果训练中没有数据缺失,而预测时出现了缺失,那么默认分到右子树.

38. xgboost防止过拟合的方式

     1. 从模型本身来讲引入了结构复杂度
     2. 增加了分裂阈值
     3. 设置参数max_depth
     4. 样本权重和小于阈值时停止建树

39. xgboost树复杂度的定义

     1. 树里面的叶子节点个数
     2. 树上叶子结点的得分w的L2正则

40. xgboost判断特征重要程度的三种方式

     1. 该特征在所有树中用做分割样本的特征的次数
     2. 在所有树中的平均增益
     3. 在树中使用该特征时的平均覆盖范围

41. RF GBDT XGB优缺点及应用场景有什么不同

42. 手写KMeans

43. Kmeans是否一定收敛

44. 对于分布不均匀的KMeans如何处理

45. 其他聚类方法

46. EM算法推导

## 图计算相关：

1. LINE算法的原理
2. GCN的原理
3. 推荐系统的冷启动
4. DeepWalk node2vec

## 其他深度学习相关：
1. RNN基本原理
2. LSTM基本原理
3. 前向传播和后向传播的区别
4. 手推后向传播
5. 神经网络中正则化的方法
6. Dropout为什么训练时使用dropout，测试时不使用

不希望在测试阶段输出结果是随机的,如果测试阶段用了dropout,预测会受到干扰

7. 深度学习中的各种Optimizer的区别

统计相关：
    1. 协方差和相关性