## 机器学习相关

### 特征工程方面：

1. 常见的机器学习指标，解释准确率、recall、precision、ROC曲线和AUC、KS曲线及KS值
    1. 准确率:对于分类问题,准确率accuracy为所有分类正确的样本数量除以总样本数量,也就是混淆矩阵的对角线和除以总数.这个指标的问题在于对于不均衡样本无法很好的反映模型精度.
    2. recall:召回率.计算公式为$\frac{TP}{TP + FN}$.分子是指分类正确的正样本,分母是指所有真实的正样本.因此召回率就是计算所有真实正样本中有多少被正确识别出来的.
    3. precision:精确度.计算公式为$\frac{TP}{TP+FP}$.分子是指分类正确的正样本,分母是所有被分为正样本的样本,因此精确度计算的是在所有分类为正样本的样本中,有多少是真正的正样本.
    4. F1Score:recall和precision的调和平均.$\frac{1}{F1} = \frac{1}{\frac{1}{recall} + \frac{1}{precision}}$即$F1 = \frac{2recall * precision}{recall + precision}$
    5. ROC曲线:ROC曲线的横轴是FPR,公式为$\frac{FP}{FP+TN}$,指的是所有真实的负样本中有多少被错误分类到正样本中.纵轴是TPR也就是recall.对于一个模型,会通过修改模型分类阈值形成一条ROC曲线,例如LR,可以通过修改正负分类阈值0.5这个值,对于不同的值会获得一个对应的(FPR, TPR)pairs,将这些pairs连成线则形成了一条ROC.因此不同的模型可以形成不同的ROC曲线.而AUC是指ROC曲线下的面积.相当于ROC曲线的量化指标,AUC越大,模型越好.
    6. KS曲线:KS曲线的横坐标为阈值,纵轴为TPR和FPR,也就是说一个KS图上是有两条曲线的,KS值即为TPR曲线和FPR曲线之间的最大差值
    
2. 样本数量不平衡时的处理方式(正样本少,负样本多)
    1. 采用上采样的方法.对于样本少的数据可以采用上采样,即让样本进行复制,但是问题是这样的复制方式会引入大量的重复数据,因此需要在复制的时候给样本增加随机扰动;
    2. 采用下采样的方法.对样本多的数据可以采用下采样,即减少负样本的数量使其和正样本数量均衡.但这样的问题是模型能够学习到的数据空间减小,模型泛化能力变差,具体的改进方法可以在模型阶段进行改善,例如用randomforest的思想或者,增量训练的思想.
    3. 采用数据生成的方式SMOTE.在邻域内随机选择一个点来生成新的点.存在两个问题:1.生成的点没有足够多的有益信息;2.会生成一些重合的样本点
    4. 通过模型来改善,例如树模型或者增量模型等.
    
3. 为什么特征解可以描述主成分
    1. 因为特征解对应的投影方差较大，对应的是方差最大化理论
    2. 小的特征方差小，类似于噪声
    
4. 过拟合以及过拟合产生的原因

    1. 过拟合是指在训练集上的表现好，但是在测试集上的表现较差
    2. 模型过于复杂
    3. 数据存在很多的噪声
    4. 数据规模不足

5. 特征选择的方式

    1. 通过Perason相关系数，互信息等计算单个特征和结果之间的关联性，排序留下topK，问题是没有考虑到特征之间的关联作用，可能把有用的关联特征误踢掉
    2. 包裹型。首先用全量特征跑一个模型，根据线性模型的系数删除一定量的特征，观察AUC变化；逐步进行，直到AUC出现大的下滑为止
    3. L1正则

6. AUC的两种计算方式

    1. 根据AUC定义来计算ROC曲线下的面积

    2. 从AUC的统计意义去计算。即在所有的正负样本对中，正样本排在负样本前面占样本对数的比例。具体做法为：
        $$
        AUC=\frac{\Sigma_{i \in positiveClass}rank_i-\frac{M(1+M)}{2}}{M*N}
        $$

        ```python
        def calAUC(probs, labels):
        	f = list(zip(probs, labels))
        	rank = [value2 for value1, value2 in sorted(f, lambda x:x[0])]
        	rank_list = [i + 1 for i in range(len(rank)) if rank(i) == 1]
        	pos_num = 0
        	neg_num = 0
        	for i in range(len(labels)):
        		if labels[i] == 1:
        			pos_num +=1
        		else:
        			neg_num += 1
        	auc = 0
        	auc = (sum(rank_list) - pos_num * (pos_num + 1) // 2) / (pos_num * neg_num)
        	return auc
        		
        ```

7. 连续特征的特征离散化作用？

    1. 表示为稀疏矩阵，容易存储
    2. 计算速度快
    3. 引入一定的非线性
    4. 具有鲁棒性

8. 连续特征的特征缩放作用

    1. 归一化可以提高计算速度
    2. 对于需要计算样本距离的模型，有可能能够提高精度

### 模型方面

1. 最小二乘法公式推导
    $$
    min\Sigma_i(y_i - Wxi)
    $$
    

2. Logistics原理及推导

    y = wx+b加一个sigmoid将输出的范围映射到0-1之间.

3. 逻辑回归为什么要特征离散化
    1. 稀疏向量内积乘法运算速度快,计算结果方便存储,容易扩展
    2. 离散化后的特征对异常数据有很强的鲁棒性
    3. 逻辑回归本身的表达能力有限,将单变量离散化为N个后,每个变量有单独的权重,引入了非线性,提升模型的表达能力
    4. 离散化后可以进行特征交叉,进一步引入非线性

4. 如果特征高度相关或者一个特征重复多遍会有什么影响？
    如果在损失函数最终收敛的情况下，不会影响最终的效果。但是对于特征本身，假设只有一个特征，不重复与重复多遍，训练结束后，后者这些特征的权重和将会等于前者。

5. 为什么我们还是要将高度相关的特征去掉？
    首先可以让模型的可解释性更好。其次可以提高训练速度，因为如果有特征高度相关，就算损失函数本身收敛，但实际参数并没有收敛，会拉低训练速度。并且特征多了，训练时间自然会提高。

6. LR如何进行并行化处理

    1. 如果是按行并行，即将样本拆分到不同的机器上去，其实就是每台机器都计算出各自的梯度，然后归并求和再求平均
    2. 如果是按列并行

7. 梯度下降法找到的一定是下降最快的方向么
    不一定,只是当前切平面的切线方向.

8. 梯度下降法,牛顿法,拟牛顿法的区别
    1. 梯度下降法是一阶优化方法,牛顿法是二阶优化的方法,需要计算Hesson阵.具体来说梯度下降法是一种迭代算法,选取适当的初值,不断根据负梯度的值进行迭代,直到收敛.
    2. 牛顿法每一步需要计算Hesse阵的逆矩阵,因为极值发生在一阶导数为0的点.牛顿法只适用于Hesse阵正定的情况,在深度学习中,如果Hesse阵的特征值不都是正的,会导致更新向错误方向移动,可以通过正则化避免,常用的正则化方式是在对角线上增加单位量.
    3. 拟牛顿法的出现是为了改进Hesse阵计算过程中的复杂度过高的问题.用一个n阶矩阵来近似Hesse阵的拟矩阵,其需要满足拟牛顿条件.

9. SVM的主要思想

    建立一个最优决策超平面,使得分类正负样本距离这个超平面距离最大化

10. 手推SVM损失函数

   损失函数来自于满足分类正确性的同时最大化几何间隔

11. 如何理解SVM中的函数间隔和几何间隔
     1. 函数间隔:通过观察wx+b与label y的符号是否一致可判断分类是否正确,y(wx+b).但是函数间隔存在一定的问题,如果成比例改变w和b,函数间隔的值将随之改变.
     2. 几何间隔:函数间隔除以||w||,直观上的点到平面的距离

12. 什么是支持向量

     间隔边界上的点是支持向量,满足$y_i(w^Tx_i+b)=1$

13. SVM的软间隔，硬间隔

    1. 硬间隔：完全分类准确，其损失函数不存在；其损失值为0；只要找出两个异类正中间的那个平面；
    2. 软间隔：允许一定量的样本分类错误；优化函数包括两个部分，一部分是点到平面的间隔距离，一部分是误分类的损失个数；

14. SVM是否需要normalization？

    不一定，需要具体问题具体分析，引入归一化可以提高计算速度，但是有时候会带来准确率的降低，而且归一化相当于引入了一个较强的假设。

15. SVM中的核函数种类

    1. 多项式核
    2. 高斯核,由$\sigma$参数控制,如果其过小,那么可以讲任意的数据映射为线性可分,但容易过拟合
    3. 线性核
    4. 核函数的价值在于虽然是将特征进行从低维到高维的转换,但它避免了在高维空间上的复杂计算,而是先在低维上进行计算,再将分类效果表现在高维上

16. SVM为什么要引入对偶

    1. 对偶问题将原始问题的不等式约束转化为对偶问题中的等式约束,更容易求解
    2. 方便核函数的引入

17. SVM中kernel的作用

    把低维数据映射到高维,使得某些低维不可分的问题可以在高维解决.

18. 为什么SVM对缺失数据敏感？

    这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。

19. LR和SVM的区别

    1. LR采用的是最大似然估计,SVM采用的是合页损失函数
    2. SVM不直接依赖数据的分布，分类平面受到一类点的影响；LR受所有数据点的影响，数据需要balance
    3. SVM可以通过核函数解决非线性问题
    4. 

20. L1正则和L2正则什么分布，数学上如何解释，如何选择

    1. L1是拉普拉斯分布, 是不完全可微的,在参数W=0点的概率最高;L2是高斯分布,是完全可微的,在9附近的概率较大
    2. L1使特征稀疏,在一定程度上实现了对特征的选择;
    3. L2只会将对模型贡献不大的特征所对应的参数置于无限小的值,因为参数值很小概率会出现在坐标轴上

21. VC维的含义

22. 防止过拟合的方式
    1. 正则化
    2. 增加数据量,减少特征维度
    3. Early Stop
    4. Dropout
    5. BN

23. 为什么要做特征归一化,作用是什么
    1. 归一化可以加快梯度下降法求解最优解的速度,当数值范围相差太大时,收敛路径呈Z字型,导致收敛太慢.
    2. 归一化可以提高计算精度.对于KNN来说,如果某个特征范围非常大,那么距离计算就主要取决于这个特征,从而与实际情况互相违背.

24. 哪些机器学习模型不需要归一化

    通常树模型不需要归一化,因为归一化并不会对树模型的特征划分产生印象

25. KNN中的K如何选取

    1. K值较小,模型复杂度较高,容易发生过拟合
    2. K值较大,模型复杂度下降
    3. 通常用交叉验证法来选取最优K值

26. 参数模型及非参数模型

     在于总体的分布是否已知

     参数模型的特点是简单，快速；缺点是复杂度有限，收到本身函数形式约束

     非参数模型的特点是可变性，模型强大；缺点是需要跟多的数据，速度慢，过拟合

27. 参数模型

     1. 逻辑回归
     2. 线性成分分析
     3. 感知机

28. 非参数模型

     1. 支持向量机
     2. 朴素贝叶斯
     3. SVM
     4. 神经网络

29. 朴素贝叶斯相关公式

30. 超参优化的方式

31. 分类树模型的评价指标

     1. 熵
     2. 信息增益
     3. 信息增益率
     4. 基尼指数

32. 回归树的评价指标

    1. 均方误差
    2. 对数误差

33. 回归树的分裂方式

    遍历所有特征的所有切分点,找到最优切分点

34. Adaboost权值更新公式

35. 决策树的剪枝方式

36. 为什么说bagging是降低了方差

     1. 由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance。
     2. $E[\frac{\Sigma X_i}{n}] = E[X_i]$
     3. $Var(\frac{\Sigma X_i}{n}) = \frac{Var(X_i)}{n}$

37. boosting和bagging在不同情况下的选用

38. GBDT推导

39. GBDT和randomforeast的区别

     1. GBDT是boosting方法,每一次训练都是为了减少上一次的残差,在GBDT中,关键就是利用损失函数的负梯度在当前模型的值作为残差的近似值,进而拟合一颗CART回归树,因为GBDT会累加所有树的结果,因此只能是回归树
     2. RF是bagging的一种,各个基分类器之间并列生成.RF是以决策树为基分类器构建,在训练过程中引入随机特征选择.
     3. RF主要包含四个部分:随机选择样本(有放回);随机选择特征;构建决策树;平均投票
     4. RF的重要特征就是不需要进行交叉验证,因为只是用了一部分样本
     5. 相比于普通的bagging,RF的训练效率较高,但是起始性能交叉.

40. 介绍GBDT和xgboost的区别及联系

     1. 传统的GBDT以CART树作为基分类器,XGBoost支持线性分类器,此时XGBoost相当于正则化的LR和线性回归
     2. GBDT在优化的时候只用到了一阶导数信息,XGBoost则对代价函数进行了二阶泰勒展开,得到了一阶及二阶导数
     3. XGBoost在代价函数中加入了正则项,用于控制模型的复杂度,防止过拟合
     4. shrinkage(缩减),相当于学习速率,在进行完一次迭代时候,会将叶子节点的权值上乘上该系数,为了削弱每棵树的影响
     5. 列抽样.借鉴随机森林的做法,支持列抽样
     6. 对缺失值的处理
     7. 并行化.是在特征粒度上的并行,使用了block结构,预先对数据进行排序.

41. GBDT和xgboost在划分时有什么区别

    1. xgboost划分是通过对每一个节点都遍历所有的特征,先按照该特征里面的值进行排序,然后线性扫描进而确定最好的分割点
    2. 对于所有的特征x，只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和GL和GR。然后用计算Gain的公式计算每个分割方案的分数就可以了。
    3. 引入分割不一定会使得情况变好，所以我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值γ 的时候，则忽略这个分割

42. xgboost的原理

43. xgboost求节点值的方式

     二次规划求最优

44. 为什么xgboost要用泰勒展开

     1. 二阶导数为了梯度下降的更快
     2. 自定义loss function

45. xgboost如何处理缺省值

     1. 把缺失值当做稀疏矩阵来对待,本身在节点分裂时不考虑缺失值的数值,会分到左子树和右子树分别计算层损失,选择较优的一个.
     2. 如果训练中没有数据缺失,而预测时出现了缺失,那么默认分到右子树.

46. xgboost防止过拟合的方式

     1. 从模型本身来讲引入了结构复杂度
     2. 增加了分裂阈值
     3. 设置参数max_depth
     4. 样本权重和小于阈值时停止建树

47. xgboost树复杂度的定义

     1. 树里面的叶子节点个数
     2. 树上叶子结点的得分w的L2正则

48. xgboost判断特征重要程度的三种方式

     1. 该特征在所有树中用做分割样本的特征的次数
     2. 在所有树中的平均增益
     3. 在树中使用该特征时的平均覆盖范围

49. RF GBDT XGB优缺点及应用场景有什么不同

50. 手写KMeans

51. Kmeans是否一定收敛

52. 对于分布不均匀的KMeans如何处理

53. 其他聚类方法

54. EM算法推导

## 图计算相关

1. LINE算法的原理
2. GCN的原理
3. 推荐系统的冷启动
4. DeepWalk node2vec

## 其他深度学习相关
1. RNN基本原理
2. LSTM基本原理
3. 前向传播和后向传播的区别
4. 梯度消失和梯度爆炸是什么，如何解决
5. 手推后向传播
6. 神经网络中正则化的方法
7. Dropout为什么训练时使用dropout，测试时不使用

不希望在测试阶段输出结果是随机的,如果测试阶段用了dropout,预测会受到干扰

7. 深度学习中的各种Optimizer的区别

## 统计相关
    1. 协方差和相关性

## 熟练手推

1. LR

2. SVM

3. 熵、交叉熵、KL散度、互信息

4. 多维高斯公式

5. AdaBoost

6. GBDT

7. xgboost

8. KNN

9. FM

10. FFM

11. DeepFM

12. PCA和SVD

    1. SVD

        1. 当矩阵不是方阵的时候，无法为其定义特征值和特征向量，可以用一个相似的概念来代替：奇异值。本身是一种矩阵分解的方法
        2. SVD公式为：$M = U\Sigma V^*$，$\Sigma$对角线上的元素为M的奇异值
        3. 如果有一个向量x，$Ax = USV^Tx$，A 是一个线性变换，把 A 分解成 USV'，S 给出了变换后椭圆长短轴的长度， U 和 V' 一起确定了变换后的方向，所以 U、S、V' 包含了这个线性变换的全部信息。S 矩阵的对角线元素称为 A 的奇异值，与特征值一样，大的奇异值对应长轴，小的奇异值对应短轴，大的奇异值包含更多信息
        4. 当矩阵是对称方阵时，其特征值和奇异值相同
    2. PCA
        1. PCA首先进行标准化，计算各维度的均值及方差
        2. 进行协方差矩阵的计算
        3. 求解特征值及特征向量
        4. 取出前面能占99%的特征值对应的特征向量形成变换矩阵
        5. 进行线性变换

13. 各类优化器

14. 回归树

15. lstm

16. GRU

